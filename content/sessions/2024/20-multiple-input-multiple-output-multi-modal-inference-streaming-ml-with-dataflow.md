---
title: "Multiple Input, Multiple output, Multi-Modal Inference: Streaming ML with Dataflow"
slug: multiple-input-multiple-output-multi-modal-inference-streaming-ml-with-dataflow
speakers:
 - Wei Hsia
topics:
 - ML
 - Use case
 - State & timers
 - Python
room: Bonsai
time_start: 2024-09-05 13:30:00
time_end: 2024-09-05 14:50:00
day: 1
gridarea: "9/5/12/6"
timeslot: 20
images:
 - /images/sessions/2024/workshop-streaming.jpg 
---

Imagine when you’re attending an event, as you park your car, you get a notification telling you which entrance has the current shortest line that’s closest to your parking. Then when you check in, a reminder (only if you’ve parked), that you can get some parking validation done if you spend on concessions or merchandise as part of your membership benefit. And you have a hankering for nachos, hoping that they don’t run out. This would make your experience amazing and unique! 

How can you do this though? You need to, in real time, connect your parking data to your membership data, to current lines? How does the event organizer know how busy it is in the lines easily? Or predict how fast they will be going to run out of things? 

You’ll build a pipeline, from scratch, that incorporates all of these things. You’ll need to put all of your Beam knowledge (or learn them along the way!) to the test. Read multiple inputs, in real time, and make sure you’re enriching them with the right information. Write multiple outputs - in real time, to actually do something with the data. And of course, we’ll show you how you can use an LLM (or any model!) in the same pipeline.