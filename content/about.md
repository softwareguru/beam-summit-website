---
title: About Beam Summit
date: '2023-02-20T07:51:25.000+00:00'
url: "/about"

---
## What is Beam Summit?

The goal of Beam Summit has been to connect a community of professionals around the world who use, contribute to, and are learning Apache Beam. The 2025 edition focused on exploring how Beam, in seamless integration with Apache Kafka, Apache Iceberg, and Machine Learning, empowers data engineers to build next-generation data solutions.

This annual conference provides a space to share use cases, performance and resource optimizations, discuss pain points, and talk about the benefits of implementing Apache Beam in organizations. The event brought together the Apache Beam community to discuss the project’s status, its technical advances, and its future.

Contents focused on sharing:

* New use cases from companies using Apache Beam
* Community-driven talks
* Technical deep dives
* In-depth workshops

## Didn’t attend in person?

You still have a chance to join us online!

We're hosting Beam Summit 2025 Online Extension on July 30, a global virtual edition featuring new content and select talks from the in-person event. Explore the full program and join from anywhere: https://beamsummit.org/program/

<a href="https://youtube.com/live/5iyiKi1kzGY?feature=share" target="_blank" class="text-decoration-none">
    <button class="button mx-auto d-block ">Join Online Event</button>
</a>

## About Apache Beam

Apache Beam is an open-source, unified model for defining both batch and streaming data-parallel processing pipelines. Using one of the open-source Beam SDKs, you can build a program that defines the pipeline. The pipeline can then be executed by one of Beam’s supported distributed processing backends, which include Apache Flink, Apache Spark, and Google Cloud Dataflow.
Apache Beam is particularly useful for embarrassingly parallel data processing tasks, in which the problem can be decomposed into many smaller bundles of data that can be processed independently and in parallel. You can also use Beam for Extract, Transform, and Load (ETL) tasks and pure data integration. These tasks are useful for moving data between different storage media and data sources, transforming data into a more desirable format, or loading data onto a new system.

Learn more about Apache Beam: https://beam.apache.org/