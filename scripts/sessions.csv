Session Id,slot,title,slug,content,email,speakers,format,track,level,speaker_ids
325566,,"Beam Playground: discover, learn and prototype with Apache Beam",beam-playground,"The talk will be aimed at presenting Apache Beam discovery and learning through Beam Playground, proof of concept and contributing paths to those who are looking to learn, use and develop Apache Beam.
 
 The planned structure of the talk will be:
 1. Discovery and learning experience. I would like to go through the Apache Beam Playground to show how to use it and how Apache Beam code can be executed without the need to install Beam SDK. It will also include the showing of different completed examples that can be run, choosing SDK for an example, and what a user can expect as the output of the execution: result output, logs, execution graph. 
 2. Local prototyping and development experience. How to run Apache Beam pipeline in local environment as a user, how to setup local Apache Beam developer environment using Docker. 
 3. Contributor experience. Here I would like to show how users can contribute to Beam Playground by adding a new example to the set of Playground examples. The following steps here will be: Registration as Apache Beam contributor; Adding a new example to Beam repository marked with a special tag for Beam’s Playground; Creating a PR for the Playground example and validating the PR; Review and merging PR; Running the new example in Beam Playground",daria.malkova@akvelon.com,Daria Malkova,Lightning talk,Community or ecosystem,Introductory and overview,bcc2f90d-d925-494e-b584-89a24b668494
326809,,Introduction to the benchmarks in Apache Beam,introduction-to-the-benchmarks-in-apache-beam,"We all know that the benchmarking is a very important but quite tough and ambiguous part of performance testing for every software system. Furthermore, benchmarking the systems that support different language SDKs and distributed data processing runtimes, like Apache Beam, is even more tough and ambiguous. 
 
In this talk we will discover what kind of benchmark frameworks Apache Beam already supports (Nexmark, TPC-DS), how they can be used for different purposes (like release testing) by developers and ordinary users, and what are the plans for the future work.",aromanenko.dev@gmail.com,Alexey Romanenko,Session 25m,Community or ecosystem,Intermediate,bab286f9-e757-471d-bc7b-f25764ba5db9
327145,,"The Ray Beam Runner Project: A Vision for Unified Batch, Streaming, and ML",the-ray-beam-runner-project,"The Ray Beam Runner Project is an initiative to create a new Pythonic Beam Runner based on the Ray distributed compute framework. The project was conceived based on strong community interest in integrating Ray with Beam, and a prototype quickly proved its viability. We will discuss the current state of this initiative, and our long-term vision to provide a unified authoring and execution environment for mixed-purpose batch, streaming, and ML pipelines.",smtp.pdames@gmail.com,"Patrick Ames, Jiajun Yao, Chandan Prasad",Lightning talk,Community or ecosystem,Introductory and overview,"fac4ff06-709a-47b4-974e-5d732e1b6aa5, dabd5bff-dd9e-488e-b00e-dee41d8cad6f, ef69c2d7-f98a-4cfd-9e42-e93573193c63"
313842,,How the sausage gets made: Dataflow under the covers,how-the-sausage-gets-made:-dataflow-under-the-covers,"The team that builds the Dataflow runner likes to say it ""just works"". But how does it really work? How does it decide to autoscale? What happens on failures? Where is state data stored? Can I have infinite state data? What goes into Dataflow's latency? What's the checkpoint logic?
 
These and other questions to be answered in this session!",polecito.em@gmail.com,Pablo Estrada,Session 50m,Deep-dive,Intermediate,b22c325c-71be-404d-ac93-19a0624215cd
325571,,Collibra’s Telemetry Backbone - OpenTelemetry and Apache Beam,open-telemetry-collibra,"At a certain time, when you’re a company that’s building out a successful SaaS platform there comes a time when classic observability tools don’t cut it anymore. Or they don’t scale as well as you grow or they become too expensive. That’s why the SRE team started to look at observability as a big data problem: In combination with a specification for telemetry (OpenTelemetry) and big data tools like Apache Beam they build the Telemetry Backbone, the streaming hub for all of Collibra’s telemetry data. This is the story and how Apache Beam is an important factor in its success.",alex.vanboxel@gmail.com,Alex Van Boxel,Session 50m,Case study,Introductory and overview,9e4677aa-cf15-414f-9546-1d15e5fcd355
320876,,Developing PulsarIO Connector,developing-pulsario-connector,"Overview of how PulsarIO Connector was implemented, how it works, and how to use it.",marco.robles@wizeline.com,Marco Robles,Session 50m,Deep-dive,Intermediate,a7b9770d-06ed-42be-92f4-c31917930b6c
326825,,Effective detecting and preventing abuse on LinkedIn with Beam streaming processing,detecting-abuse-beam-streaming-processing,"The anti-abuse team at Linkedin builds data infrastructure that generates offline datasets. Data is essential to our abuse defense mechanisms such as machine learning models or rule-based anti-abuse systems. Due to the adversarial nature of the problem, the defenses generated from the offline datasets need to be updated frequently in order to detect and prevent abuse activities that are rapidly changing and evading. The paradigm shift to Beam streaming data processing opened a new door to the team and empowered us to defeat sophisticated abusers quickly and accurately. In this talk, we will discuss the use case of defending automated scraping abuse with the Beam model. We will share what the anti-abuse challenges are with the offline dataset, how Beam helps to solve the problems, and how we benefit from the adaptation of Beam’s programming model and framework in our anti-abuse defense.",ruhan@linkedin.com,Rui Han,Session 25m,Case study,Intermediate,d1f38faa-a741-4be2-813b-b9fa47fd1696
326704,,Detecting Change-Points in Real-Time with Apache Beam,detecting-change-points-in-real-time,"Apache Beam provides an expressive and powerful toolset for transformation over bounded and unbounded (streaming) data. One common but not obvious transformation that Oden has needed to implement is Change-Point Detection.
 
Change-Point Detection is at the heart of many of Oden's real-time features to its manufacturing clients. For example, many clients need to track when their factory’s production lines stop running in order to root-cause issues and improve capacity.
 
Oden continuously streams sampled real-world properties of the machines used in those lines, such as motor-speed, and uses change-detection to differentiate periods where the line was “stopped,” “ramping up,” or “running.” This problem is complicated by the need to “smooth out” flapping stoppages and by sparse, late, and out-of-order data caused by network outages.
  
In this talk, we'll cover different methods of change-point detection in bounded and unbounded PCollections using Beam Windows and State, how to implement ""smoothing"" of change-points to limit their frequency, and the implications of event sparsity, lateness, and order on these methods.",Devon.peticolas@gmail.com,Devon Peticolas,Session 25m,Deep-dive,Intermediate,a4728633-1707-4c61-9f82-b80d40d390fc
313169,,Improving Beam-Dataflow Pipelines for Text Data Processing,improving-beam-dataflow-pipelines-for-text-data-processing,"In this session, we'll share a few recipes to improve Beam-Dataflow pipelines when dealing with sequence data. These methods came from our experience of processing and preparing large datasets for ML use at Carted. We'll provide a step-by-step framework of how to analyze the issues that can start surfacing when processing text data at scale and will share our approaches to dealing with them.
 
We hope you'll apply these recipes to your own Beam-Dataflow pipelines to improve their performance. 
 
Some topics that we'll cover in this session have been discussed in our blog post: https://www.carted.com/blog/improving-dataflow-pipelines-for-text-data-processing/. The accompanying GitHub repository of the blog post is available here: https://github.com/carted/processing-text-data.",sayak@carted.com,"Sayak Paul, Nilabhra Roy Chowdhury",Session 50m,Deep-dive,Intermediate,"52e6a833-f6a7-4d7c-a8d4-bb1cbfdc0a14, 9273f9a7-4ee9-41b1-86e9-9e5b0ccde904"
326340,,Implementing Cloud Agnostic Machine Learning Workflows with Apache Beam on Kubernetes,implementing-cloud-agnostic-ml,"The need for a highly efficient data processing workflow is fast becoming a necessity in every organization implementing and deploying Machine Learning models at scale. In most cases, ML teams leverage the managed service solutions already in place by the cloud infrastructure provider they choose. While this approach is good enough for most teams to get going, the long-term cost of keeping the platform running may be prohibitively higher over time.
 
As an alternative, we run our ML pipeline tasks as Apache Beam jobs orchestrated with Argo on Kubernetes. Using Kubernetes gives us a clean abstraction of the underlying compute resources and enables us to declaratively configure Apache Beam job runners for either streaming or batch workloads on any Cloud or OnPrem compute infrastructure.
 
In this talk, we will discuss how we have implemented a continuous integration and deployment environment stack to containerize and deploy Argo workflows for running our beam job on Kubernetes. We will go through the challenges we encountered and lessons learned with recommended best practices to consider for any MLOps team considering this approach",charles@mavencode.com,"charles adetiloye, Alexander Lerma",Session 50m,Deep-dive,Intermediate,"97aabe51-fd95-408a-8f83-10ed08d78123, 226acbb9-8f6b-4210-86c6-7031642e6361"
325711,,Protecting the Internet at Scale,protecting-the-internet-at-scale,"At BlueVoyant, we process billions of cyber events per second in an effort to secure the supply chains of several high-stakes Fortune 500 companies. Cyber attacks exploit the weakest links in a target company, and modern tech infrastructures are composed of more third-party dependencies than ever before. Our Third-Party Risk (3PR) product requires us to monitor, analyze, and pinpoint weaknesses throughout hundreds of thousands of third party entities, each consisting of tens to millions of assets that may be exploited. We do this by stream-processing billions of cyber events collected across the entire internet from a variety of disparate data sources and formats, using Apache Beam on Google Dataflow, in a project we call Prophet.
 
In this talk we'll discuss how we turned Prophet into a reality, focusing on four major computational challenges we overcame: indexing disparate cyber event data in a common data warehouse, ingesting billions of events per second, searching and ETL-ing petabytes of results in a distributed stream pipeline, and running advanced cyberanalytics on this data on-demand. As a result of these efforts, Prophet can identify the many points of weakness in an enormous and complex ecosystem of third-party dependencies, and our team of expert cyber analysts are able to respond to emerging cyber attacks immediately by querying Prophet for vulnerability and exploit fingerprints within petabytes of data going back over a year of history.",alfredo.gimenez@bluevoyant.com,Alfredo Gimenez,Session 50m,Case study,Intermediate,44fb10ab-c0b7-455e-b41f-abf6c1d08b39
325736,,Speeding up development with Apache Beam (Adobe Experience Platform),speeding-up-development-with-apache-beam,"At the core of Adobe Experience Platform (AEP), there is a large Apache Kafka deployment: 20+ data centers, 300+ billion messages a day. We use it to import/export data for external customers and integrate internal solutions. Processing those events involves lots of boilerplate code and practices: understanding the streaming platform, optimizing for throughput, instrumenting metrics, deploying the service, alerting, and monitoring.
 
Out of these requirements, we built a team to construct and be responsible for the infrastructure where Apache Beam jobs would run. Today, this allows AEP teams to quickly get their projects to a ""Production Ready"" state by decoupling the runtime (Beam, Flink, Kubernetes) from business logic, simplifying Production support (failure tolerance, scaling, and observability), and allowing customization by adding business logic in the form of user provided compiled and packaged code.
  
We achieved the following KPIs for the organization: 
 1. little time and effort to launch a Beam pipeline across multiple data centers 
 2. no code written for simple pipelines (expressed fully via a JSON-based DSL) 
 3. low count of use cases not satisfied by the streaming service mentioned above",scacun@adobe.com,Constantin Scacun,Session 25m,Case study,Introductory and overview,d2185619-1c98-4ac1-8539-b877d6bde185
317719,,Strategies for caching data in Dataflow using Beam SDK,strategies-for-caching-data-in-dataflow-using-beam-sdk,This session discusses different strategies for caching data in Dataflow using the Beam SDK,khanz@google.com,Zeeshan .,Session 25m,Deep-dive,Intermediate,10a2972d-0bc3-4542-8cad-97fe178cbf06
325004,,Sculpting Data for Machine Learning,sculpting-data-for-machine-learning,"In the contemporary world of machine learning algorithms - “data is the new oil”. For the state-of-the-art ML algorithms to work their magic it’s important to lay a strong foundation with access to relevant data. Volumes of crude data are available on the web nowadays, and all we need are the skills to identify and extract meaningful datasets. This talk aims to present the power of the most fundamental aspect of Machine Learning - Dataset Curation, which often does not get its due limelight. It will also walk the audience through the process of constructing good quality datasets as done in formal settings with a simple hands-on Pythonic example. The goal is to institute the importance of data, especially in its worthy format, and the spell it casts on fabricating smart learning algorithms.",grover.jigyasa1@gmail.com,"Jigyasa Grover, Rishabh Misra",Session 50m,Deep-dive,Intermediate,"6b154a22-5948-4445-a2d5-34f03f9f2887, 2bddc8d1-21a7-4fb9-96e8-e4b2f4d74f06"
326819,,Powering Real-time Data at Intuit: A Look at Golden Signals powered by Beam,powering-real-time-data-at-intuit,"The Stream Processing Platform powers real-time data applications at Intuit using the Apache Beam framework. The platform makes it easy for an engineer to access, transform, and publish streaming data with turn-key solutions to debug and deploy their streaming applications on a managed platform in a manner that is compliant with and leverage Intuit central capabilities. Since launch 3 years ago, the platform has grown to over 150 pipelines in production and handled at peak ~17.3 billion events and 82 TB of data. Most of our pipelines use Kafka as source, with Apache Flink as our stream processing runner of choice for executing the Beam pipelines.
 
In this talk, we will discuss how we’ve built an internal, self-serve stream processing platform with Apache Beam SDK, running on Kubernetes. Specifically, we will highlight our stream processing platform’s benefits and tech stack where Apache Beam serves as a critical technology in how we do real time data processing at Intuit. 
  
We will highlight one streaming use case that generates golden signals for all the services behind Intuit API gateway to provide visibility into the business critical systems. We will provide a high-level design overview as well as dive deep into key technical aspects related to the pipeline, such as triggers, error recovery, side inputs, and late data handling",omkar_deshpande@intuit.com,"Omkar Deshpande, Dunja Panic, Nick Hwang, Nagaraja Tantry",Session 50m,Case study,Intermediate,"637d0f88-4778-4b91-9832-cf3a7e75e0e2, 03416d1e-3eea-479f-93d9-d2aa1815f5b6, 20ddb757-6285-45ef-a811-47d00a60d4bc, a69ea7e5-5ac3-444b-863b-746aab7c07a5"
326663,,"Houston, we've got a problem: 6 principles for pipelines design taken from the Apollo missions",6-principles-for-pipeline-design,"Based in the inspiring talk about the Apollo XI ""Light years ahead"", in this talk we will cover 6 design principles for data pipelines, with a focus on streaming pipelines. Submitting a job to a runner is not that different (well, a bit) from ""submitting"" a spacecraft into space. You lose direct access to the device, and can only send a couple of commands, read logs and receive telemetry. The 6 principles presented in this talk will make your pipeline to land on the moon rather than crashing!",ihr@google.com,"Israel Herraiz, Paul Balm",Session 50m,Community or ecosystem,Intermediate,"50ccb941-18d7-4dbc-bde2-0747bb4196dc, 43455528-38f9-4d94-8459-5756f3d804cd"
327074,,Optimizing a Dataflow pipeline for cost efficiency: lessons learned at Orange,optimizing-cost-efficiency,"This session would be co-presented between a customer (Orange, french telco) and myself following a work we did together (PSO engagement between Google and Orange).
 
 It would go like this: 
 - The use case: what are we trying to do with this project (in particular, it's a project that collects all logs from internet boxes)
 - Configuration (tuning vCPUs, number of threads, enabling or disabling streaming engine)
 - Autoscaling (give some info about helping the autoscaler to behave correctly, and iteratively choose a good number of initial and max workers)
 - Writing to BigQuery: talking about the BQ Storage Write API and the BQ Load API in the context of Dataflow
 - Profiling and improving the code (basically profiling and best practices)
 
 This would be told as a story and every time we would say if each step made us gain on the cost/performance side or not.
 
 I am hesitating between 25mn and 50mn, but I believe 40mn with 10mn questions could be good. Do you think this would be a case study or a deep dive? I'm leaning towards case study.",jeremiegomez@google.com,"Jérémie Gomez, Thomas Sauvagnat",Session 50m,Case study,Intermediate,"6e7559ea-74ea-4c84-bebc-ff0fd1264ee1, c977b180-bffd-4a0c-9fbd-edfe68479a3d"
326546,,New Avro serialization and deserialization in Beam SQL,avro-serialization-deserialization,"At Palo Alto Networks we heavily rely on Avro, using it as the primary storage format and use Beam Row as in memory. We de/serialize billions Avro records per second. One day we realized Avro Row conversion routines consume much of CPU time. Then the story begins ....",talat@uyarer.com,Talat Uyarer,Session 50m,Deep-dive,Intermediate,ad4d2fbd-eeb5-4626-b174-5dd68774dc96
327097,,RunInference: Machine Learning Inferences in Beam,runinference,"Users of machine learning frameworks must currently implement their own PTransforms for predictions or inferences. Only TensorFlow makes a RunInference beam transform available, but it's not highly accessible since it's hosted in the TFX-BSL repo.
 
We are creating implementations of RunInference for two popular machine learning frameworks, scikit-learn and PyTorch. These will take advantage of both internal optimizations like shared resource models, as well as framework-specific optimizations such as vectorization. It will have a clean simple unified interface, and use types intuitive to developers for inputs and outputs (numpy for scikit-learn, Tensors for PyTorch).  
 
The eventual goal is to support this for many more ML frameworks (e.g. XGBoost, mxnet, Statsmodels, JAX, TensorRT) and remote services (e.g. Vertex AI).",yeandy@google.com,Andy Ye,Session 25m,Community or ecosystem,Introductory and overview,b0aa5f2c-04e3-45b2-8d47-e7d83a0f75ff
327149,,Log Ingestion and Data Replication at Twitter,log-ingestion-replication-twitter,"Data Analytics at Twitter rely on petabytes of data across data lakes and analytics databases. Data could come from log events generated by twitter micro services based on user action(in the range of trillions of events per day) or data is generated by processing jobs which processes the log events. The Data Lifecycle Team at twitter manages large scale data ingestion and replication of data across twitter data centers and public cloud. Delivering the data either in streaming or batch fashion to data lakes(HDFS, GCS) and data warehouse(Google BigQuery) in a reliable and scalable way at lowest possible latency is a complex problem. In this talk, we will explain our log ingestion architecture and data replication architecture across storage systems and explain how we use beam based ingestion/replication pipelines for both batch and streaming use cases to achieve our goal.",pkillamsetti@twitter.com,"Praveen Killamsetti, Zhenzhao Wang",Session 50m,Deep-dive,Introductory and overview,"383e4024-3fac-4203-962a-9a9db35f31be, 21c36ec5-21d9-4d0f-908b-1671d4c67e7d"
318362,,Online clustering and semantic enrichment of textual data with Apache Beam,online-clustering-and-semantic-enrichment-of-textual-data,"In this talk, we will be looking at some Apache Beam design patterns and best practices that you can use to successfully build your own Machine Learning workflows. We will be looking at examples of applying Apache Beam to solve real-world problems, such as real-time semantic enrichment and online clustering of text content. We will also discover how to use Beam efficiently as an orchestrator for Machine Learning services and how to design your entire system with scalability in mind.",alexandru.balan@ml6.eu,Alexandru Balan,Session 50m,Deep-dive,Advanced,e5c8d474-dffa-44f6-9a84-602189f0d898
327126,,Use of shared handles for Cache reuse across DoFn’s in Python,shared-handles-cache-reuse,The session will talk about how we use shared handles to enrich our events with metadata using shared handles.,amruta.deshmukh@strivr.com,Amruta Deshmukh,Session 25m,Case study,Introductory and overview,631084c3-712a-4c63-995e-4d6416f74129
324324,,Supporting ACID transactions in a NoSQL database with Apache Beam,supporting-acid-transactions-in-nosql,"In this session we will see how we can use Apache Beam to enhance a generic eventually consistent NoSQL database (e.g. Apache Cassandra) by ACID transactions. We will see how we can use gRPC with splittable DoFn to create an RPC streaming source of requests into Apache Beam Pipeline and then how we can process these requests inside the Pipeline ensuring the requests are applied atomically, consistently, independently and durably despite the data being backed by an eventually consistent data store. The session will use a simplified scenario of banking transactions as a motivating example.",je.ik@seznam.cz,Jan Lukavský,Session 25m,Case study,Intermediate,a41a7bda-3321-4a7f-ae78-a9c15470ef38
326772,,"Relational Beam: Process columns, not rows!",relational-beam,"Last year we kicked off relational work with a vision of automatically optimizing your pipeline. Now we have a panel of contributors who are working towards this goal! We will demo the new optimizer in Java core, showing how we can automatically prune columns from IOs. Then we will discuss our upcoming work to make vectorized execution a reality through native columnar support in Python and Java. Finally we will discuss usage best practices around using Schemas, Dataframes, and SQL to ensure you can benefit from these changes.",apilloud@apache.org,"Andrew Pilloud, Brian Hulette, Kyle Weaver",Session 50m,Deep-dive,Intermediate,"a23b68d7-5ff5-4c05-aded-30ce209d9317, 702f0896-aa45-4923-9918-7920af38fa9a, fdab9b2b-e199-4834-aaf9-3b823e6c5b28"
327106,,Unified Stream and Batch Pipelines at LinkedIn using Beam,unified-stream-and-batch-pipelines-at-linkedin-using-beam,"Many use cases at LinkedIn require real-time processing and periodic backfilling of data. Running a single codebase for both needs is an emerging requirement. In this talk, we will share how we leverage Apache Beam to unify Samza stream and Spark batch processing. We will present the first unified production use case Standardization. By leveraging Beam on Spark for its backfilling, we reduced the backfilling time by 93% while only using 50% of resources. We will also go through the challenges of running unified pipelines, lessons we have learned, and future roadmap at Linkedin.",shanzhang@linkedin.com,"Shangjin Zhang, Yuhong Cheng",Session 25m,Case study,Intermediate,"c735b530-cd55-45db-a38d-f3b9bc4422c7, b5b11ddf-08e3-46d1-9432-60bd439d8999"
318571,,Playing the Long Game - Transforming Ricardo's Data Infrastructure with Apache Beam,transforming-ricardos-data-infrastructure,"The Data Intelligence Team at Ricardo, Switzerland's largest online marketplace, is a long-time Apache Beam user.
 
 Starting on-premise, we transitioned from running our own Apache Flink cluster on Kubernetes with over 40 streaming pipelines written in Apache Beam's Java SDK to Cloud Dataflow. This talk describes the obstacles and discusses our learnings along the way.",tobias.kaymak@gmail.com,Tobias Kaymak,Session 25m,Case study,Introductory and overview,f92be545-6d60-4cd3-946a-787776093030
327017,,Streaming NLP infrastructure on Dataflow,streaming-nlp-infrastructure-on-dataflow,"Trustpilot is an e-commerce reviews platform delivering millions of new reviews to businesses each week. We are using Apache Beam on GCP Dataflow to deliver real-time streaming inferences with the latest NLP transformer models.
 
 Our talk will touch on:
 - Infrastructure setup to enable Python Beam to interface with Kafka for streaming data
 - Taking advantage of Beam's unified programming model to enable batch jobs for backfilling via BigQuery
 - Working with GPUs on Dataflow to speed up local model inference
 - MLOps: Using Dataflow as part of a continuous evaluation model monitoring setup",ayb.chan@gmail.com,"Alex Chan, Angus Neilson",Session 50m,Case study,Intermediate,"de90f0b2-fa86-46f7-ab14-8a2096e6f931, 1692ab9a-e633-45bd-84e4-0ff9fee70410"
327123,,Scaling up pandas with the Beam DataFrame API,scaling-up-pandas-with-the-beam-dataframe-api,"Beam’s Python SDK is incredibly powerful due to its high scalability and advanced streaming capabilities, but its unfamiliar API has always been a barrier to adoption. Conversely, the popular Python pandas library has seen explosive growth in recent years due to its ease of use and tight integration with interactive notebook environments, but it can only process data with a single node - it cannot be used to process distributed datasets in parallel. In this talk I will demonstrate how Beam’s pandas-compatible DataFrame API provides the best of both tools. First, I will demonstrate how the API can be used to interactively build data pipelines that can be easily scaled up to process distributed datasets. Then, I will dive into the internals of the Beam DataFrame API and show how it scales up pandas to process distributed datasets.",bhulette@google.com,Brian Hulette,Session 50m,Deep-dive,Intermediate,702f0896-aa45-4923-9918-7920af38fa9a
324706,,"Beam Cross Language Transforms in Python, with Google Cloud Dataflow",beam-cross-language-transforms,"This three-hour workshop is intended for Beam practitioners who wish to advance their knowledge of Apache Beam and Google Cloud Dataflow. 
 
In this workshop, the attendees will be implementing a streaming pipeline in Python. The pipeline will feature the new cross language support in Beam and Dataflow. Cross language support allows access to other Beam transforms without having to introduce a new language into your environment. 
 
The streaming pipeline will pick up Cloud Pub/Sub messages and deliver them to Google Cloud BigQuery in real time. The cross language transforms will allow the author to invoke Java transforms from within a Python pipeline.  
 
The attendees will write their own pipelines with guidance from the workshop facilitators and have the opportunity to deploy it in Google Cloud using the Google Cloud Dataflow runner.",weihsia@google.com,"Wei Hsia, Israel Herraiz, Sergei Lilichenko",Workshop 3h,Deep-dive,Intermediate,"49b668aa-7b10-4651-b843-ea0092231d1b, 50ccb941-18d7-4dbc-bde2-0747bb4196dc, 5c405742-983b-475c-9de2-4790337db4e4"
325338,,Apache Beam on Amazon Kinesis Data Analytics (KDA),beam-on-amazon-kinesis-data-analytics,"In this workshop, we explore an end to end example that combines batch and streaming aspects in one uniform Apache Beam pipeline. We start to analyze incoming taxi trip events in near real time with an Apache Beam pipeline. 
 
We show how to archive the trip data to Amazon S3 for long term storage. We subsequently explain how to read the historic data from S3 and backfill new metrics by executing the same Beam pipeline in a batch fashion. Along the way, you also learn how you can deploy and execute the Beam pipeline with Amazon Kinesis Data Analytics in a fully managed environment.
 
Key areas covered in the workshop are: 
 * Create Ingestion Infrastructure
 * Deploy Streaming Pipeline 
 * Beam on KDA
 * Deploy Batch Pipeline
 * Monitoring and Profiling
 
The workshop is aimed at engineers and technical/solutions architects and the overall duration for the workshop will be 2 hrs.",amarsur@amazon.co.uk,"Amar Surjit, Subham Rakshit",Workshop 3h,Deep-dive,Intermediate,"45904f33-63d8-4724-9349-cf8054007918, f44174c6-828e-48b0-8e92-b2badbffbe11"
326683,,Splittable DoFns in Python: a hands-on workshop,splittable-dofns-in-python,"In this workshop we will review the concept of Splittable DoFns and we will write two I/O connectors using this kind of DoFns: one in batch (for reading large files in a given format), and one for streaming (for reading from Kafka topics). We will run some examples on Google Cloud Dataflow after implementing these connectors from scratch during the workshop. All the code will be written in Python.",ihr@google.com,"Israel Herraiz, Miren Esnaola",Workshop 3h,Deep-dive,Advanced,"50ccb941-18d7-4dbc-bde2-0747bb4196dc, ef9c6e6b-bcc8-41a5-99d9-735c341d6af6"