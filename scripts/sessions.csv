title,description,speakers,Session length,topics,room,time_start,duration,time_end
"Exabyte-scale Streaming Iceberg IO with Beam, Ray, and DeltaCAT","Production case study highlighting how Amazon uses Ray and DeltaCAT at exabyte-scale to resolve longstanding performance & scale challenges integrating streaming pipelines with Apache Iceberg. Highlights how the Apache Beam, Ray, Apache Flink, and Apache Spark communities can start bringing the same benefits to their workloads using the DeltaCAT project's IO source/sink implementations for Apache Beam.",Patrick Ames,25 minutes,"Connecting disparate systems with modern data architectures, Scalability & Performance, Ecosystem & Community, Emerging trends",Horizon Hall,2025-07-08 09:45:00,30,2025-07-08 10:15:00
Scaling Real-Time Feature Generation Platform @Lyft," At Lyft, real-time feature generation is crucial for powering many business critical use-cases. This session describes how we leveraged Apache Beam to build a robust and scalable real-time feature generation platform for this purpose, capable of generating 100s of millions of features per minute. We will delve into the critical factors that engineering teams should consider when designing a real-time feature generation platform, such as:
Data consistency and accuracy, with a focus on ownership and quality guarantees.
Latency requirements 
Performance optimization to ensure efficient feature serving.
Feature serving and downstream model execution pipelines.
Data lineage tools for improved traceability.
Strategies for designing for performance and minimizing infrastructure costs.
The presentation will discuss engineering challenges encountered while scaling the Beam pipeline to support our requirements and the lessons we learned along the way.
",Rakesh Kumar,25 minutes,"Real-time data applications, Unified Data Processing",Horizon Hall,2025-07-08 10:45:00,25,2025-07-08 11:10:00
A Tour of Apache Beam’s New Iceberg Connector,"Join us for a comprehensive overview of Apache Beam’s Iceberg connector. This session will cover its current features and support across multiple SDKs (Java, Python, YAML, SQL). We’ll delve into key design decisions and share what’s coming next. We'll also demonstrate how to use the connector in batch, streaming, and multi-language scenarios to build robust and scalable pipelines for your data lakehouse.",Ahmed Abualsaud,25 minutes,Connecting disparate systems with modern data architectures,Palisades,2025-07-08 10:45:00,25,2025-07-08 11:10:00
Integration of Batch and Streaming data processing with Apache Beam,"Mercari utilizes Apache Beam for batch and streaming processing for various purposes, such as transferring data to CRM and providing incentives to users.
To avoid having to develop similar data pipelines in different departments within the company, Mercari Pipeline has been developed and released as OSS as a tool that allows users to build pipelines by simply defining the processing via JSON or YAML.
(https://github.com/mercari/pipeline)

In this session, we will introduce an example of utilizing the features of Apache Beam, which allows Batch and Streaming processes to share the same code, to divert time-series aggregate values generated and verified by Batch to Streaming processes.

* Integration of multiple data sources
* Aggregate computation with window function using State API
* Aggregate computation with window function utilizing external data store (Bigtable) and CDC
* Inference with ONNX model
* Configuration management by Batch and Streaming difference",Yoichi Nagai,25 minutes,"Real-time data applications, Unified Data Processing",Palisades,2025-07-08 11:15:00,25,2025-07-08 11:40:00
Managed transforms - power of Beam without maintenance overheads,"Apache Beam offers a number of powerful transforms including a set of highly scalable I/O connectors. Usually, you have to regularly keep upgrading Beam to get critical fixes related to such transforms. With the recent addition of Java and Python managed APIs, Beam allows runners to fully manage supported transforms. Google Cloud Dataflow uses this API to manage and automatically upgrade widely used Beam I/O connectors. This allows you to focus on the business logic of your batch and streaming pipelines without worrying about associated maintenance overheads.",Chamikara Jayalath,25 minutes,"Real-time data applications, Scalability & Performance, Unified Data Processing",Palisades,2025-07-08 11:45:00,25,2025-07-08 12:10:00
From taming energy market data to hyperparameter hunting at scale: leveraging Apache Beam & BigQuery,"This session explores how Apache Beam handles the end-to-end workflow for Italian energy market analytics, including forecasting electricity demand, natural gas demand, and renewable generation.

We'll demonstrate its power as a robust ETL tool for parsing and processing diverse data sources, using Google BigQuery as the central data warehouse. These sources range from millions of XML files detailing point-of-delivery (POD) electricity demand to ECMWF-generated GRIB meteorological files, among others.

Building on this foundation, the session covers Beam's role in scalable machine learning, detailing its use for distributed Bayesian hyperparameter searches over thousands of models, efficient retraining with optimized parameters over newly-parsed data, and large-scale distributed inference.","Nicholas Bonfanti, Matteo Pacciani",25 minutes,"Scalability & Performance, Unified Data Processing",Horizon Hall,2025-07-08 11:45:00,25,2025-07-08 12:10:00
Remote LLM Inference with Apache Beam: Practical Guide with Gemini and Gemma on Vertex AI,"Large Language Models offer powerful capabilities for data transformation, but reliably integrating them at scale into Apache Beam data pipelines presents challenges. Deploying powerful, large models (e.g., Gemma 27B, Llama 70B, DeepSeek R1) directly onto Beam workers via the RunInference API is often infeasible due to resource constraints, multi-GPU complexity, cost, and lack of serving optimizations. Furthermore, many frontier models like Gemini are only available via APIs. Therefore, this session focuses on effective Remote LLM inference integration with Apache Beam.

This practical session guides you through implementing LLM pipelines using Python and Apache Beam's RequestResponseIO feature. It will demonstrate building robust callers for remote endpoints, using the native Gemini API and Vertex AI Prediction API (hosting Gemma) as concrete examples. Learn essential performance tuning techniques crucial for managing latency, throughput, and reliability in these I/O-bound pipelines. Finally, discover compelling use cases and examples for building intelligent, scalable data processing solutions with Gemini and Gemma models.",Taka Shinagawa,25 minutes,Emerging trends,Palisades,2025-07-08 12:45:00,25,2025-07-08 13:10:00
Scalable Drug Discovery with Apache Beam: From R-Groups to Crystal Structures,"In this talk, we'll share how we use Apache Beam at Schrodinger to power key stages of the drug discovery pipeline, from R-group enumeration in lead optimization to crystal structure determination in drug formulation. Rather than relying on existing runners, we built our own execution engine on top of Beam’s powerful abstraction layer to better serve our domain-specific needs.",Joey Tran,25 minutes,,Horizon Hall,2025-07-08 14:00:00,25,2025-07-08 14:25:00
Leveraging Apache Beam for Enhanced Financial Insights,"Credit Karma leverages Apache Beam to address a broad spectrum of data processing requirements, particularly real-time data transformation to bolster machine learning models. Key applications include:
1. preprocessing data and constructing graphs for live model scoring,
2. large-scale ETL (Extract, Transform, Load) operations for analytics, and
3. real-time aggregation of features to furnish near-instantaneous insights to models
we would be talking above one usecase from each pillar","Raj Katakam, Naresh Kumar Kotha, Venkatesh Poosarla",50 minutes,Real-time data applications,Palisades,2025-07-08 14:00:00,55,2025-07-08 14:55:00
Bridging BigQuery and ClickHouse with Apache Beam: A Google Dataflow Template for Batch Ingestion,his,Bentsi Leviav,25 minutes,"Real-time data applications, Ecosystem & Community, Scalability & Performance",Palisades,2025-07-08 15:00:00,25,2025-07-08 15:25:00
Superpowering Agents with Apache Beam,"Large language models and agentic systems are rapidly changing how exploratory data analytics is performed.
Natural language interactions and assisted querying unlock quick prototyping for business experts without data science or programming backgrounds enabling them to get answers to business questions faster.

The limited context inherent in LLMs prevents direct analysis of large data volumes.
Augmenting them with access to powerful tools, like databases, and query engines, and scalable data processing frameworks, overcomes this constraint.

This talk presents a use case of an agentic LLM that augments its capabilities by generating and running custom Apache Beam pipelines when the business insights require massive scale data processing.","Konstantin Buschmeier, Jasper Van den Bossche",25 minutes,Emerging trends,Horizon Hall,2025-07-08 15:45:00,25,2025-07-08 16:10:00
Simplified Streaming Anomaly Detection with Apache Beam's Latest Transform,"This talk dives into the practical application of Apache Beam for constructing robust, scalable, and real-time anomaly detection pipelines. We'll explore Beam's latest anomaly detection transform, and how to integrate various anomaly detection algorithms (e.g., statistical methods, machine learning models) to identify critical outliers in continuous data streams. ",Shunping Huang,25 minutes,Real-time data applications,Palisades,2025-07-08 16:15:00,25,2025-07-08 16:40:00
Real-Time Medical Record Processing,"At Azra AI, we help with the treatment journeys of cancer patients.  Beam is a core component of our platform allowing us to process medical records in 'real-time'.

In this talk, we will share how we leverage Beam, along with a mix of other services, to help us achieve real impactful results.  

",Austin Bennett,25 minutes,"Connecting disparate systems with modern data architectures, Real-time data applications, Ecosystem & Community",Palisades,2025-07-08 16:45:00,25,2025-07-08 17:10:00
Choosing The Right Boat For Your Stream,"Processing real-time data streams requires navigating a growing landscape of connective tissue that can move and transform data. The choices require careful consideration of tradeoffs across scalability, latency, capability, and operational overhead. These types of considerations also drive decision making in building the tools themselves. In this talk, Kamal will draw upon over a decade of building large-scale streaming services within Google Cloud to help you successfully architect your streaming data pipelines so that you aren't left wishing you had brought a bigger boat.",Kamal Aboul-Hosn,50 minutes,"Connecting disparate systems with modern data architectures, Scalability & Performance, Real-time data applications",Horizon Hall,2025-07-09 09:00:00,30,2025-07-09 09:30:00
Efficient LLM-Based Data Transformations Transformations via Multiple LoRA Adapters,"LLM-based data transformations are very powerful for processing unstructured data, organizations often struggle to deploy these tools at scale, particularly when tailoring them to specific custom use cases. This talk will explore how to efficiently serve multiple LoRA (Low-Rank Adaptation) adapters on a single base model, enabling task-specific transformations within Apache Beam pipelines and addressing the scalability challenges head-on.
LoRA adapters enable efficient fine-tuning of large language models by updating only a subset of parameters, making it possible to tailor models for specific tasks without the computational overhead of full fine-tuning. This approach is particularly valuable when working with private data or deploying cost-effective models.
We will explore how inference servers like vLLM and NVIDIA NIM can dynamically swap LoRA adapters in real-time, optimizing resource utilization while seamlessly integrating with Apache Beam for batch and streaming data processing. This integration ensures scalable, cost-effective, and adaptable workflows for various data sources.
We'll demonstrate this approach through a real-world implementation where we process different document types using custom LoRA adapters for each—from invoices to legal contracts—achieving specialized extraction capabilities while maintaining a single model infrastructure.
The talk will cover: (1) an overview of LoRA adapters and their efficiency benefits, (2) configuring inference servers for dynamic adapter swapping, and (3) implementing a complete Apache Beam pipeline for production-ready unstructured data processing.",Jasper Van den Bossche,25 minutes,"Real-time data applications, Scalability & Performance",Palisades,2025-07-09 10:30:00,25,2025-07-09 10:55:00
Building Banking Synthetic Data for a Lakehouse with Gemma,"Building a Beam pipeline to preprocess, generate and validate Synthetic data for a Datawarehouse migration into GCP.

Why It’s New: Synthetic data generation is a hot topic, and using GenAI with Beam is a novel approach, we were inspired by https://developers.googleblog.com/en/gemma-for-streaming-ml-with-dataflow/ and decided to use Beam for preprocessing, generation and validation to scale synthetic data generation from 1 up to 3000 tables and leverage the model forkeeping primary keys referencial integrity among them.

Tech Stack: Apache Beam, TensorFlow, Google Cloud.",Alberto López Serna,25 minutes,Emerging trends,Horizon Hall,2025-07-09 10:30:00,25,2025-07-09 10:55:00
Talk to your pipeline: how to use AI to create dynamic transforms in streaming,"With some design, you can leverage Beam ML to make a LLM transform your questions into dynamic transformations that you can apply to your data. In this talk, we show how to use Gemma to accept natural language questions about your data, and offer an answer in real time applied to the main stream of data","Israel Herraiz, Kfir Naftali",25 minutes,"Real-time data applications, Emerging trends",Horizon Hall,2025-07-09 11:00:00,25,2025-07-09 11:25:00
Enhancing Data Quality for AI Success,"""Enhancing Data Quality for AI Success"" focuses on the critical role that high-quality data plays in the effectiveness and accuracy of AI models. Since AI systems learn patterns from data, ensuring that the data is clean, diverse, accurately labeled, and regularly updated is essential for optimal performance. Poor-quality data can lead to inaccurate predictions, biased results, and underperforming models. By implementing strategies like data cleansing, augmentation, and proper annotation, organizations can improve the training process, resulting in more reliable, fair, and effective AI systems. The topic emphasizes that the success of AI initiatives depends as much on the data used as on the algorithms themselves.",Aarohi Tripathi,25 minutes,,Palisades,2025-07-09 11:00:00,25,2025-07-09 11:25:00
Introduction to the Apache Beam RAG package,"Use the extensible Apache beam RAG package to build pipelines that 
- Generate vector embeddings
- Ingest embeddings to desired vector database
- Perform semantic search",Claude van der Merwe,25 minutes,"Emerging trends, Ecosystem & Community",Horizon Hall,2025-07-09 11:30:00,25,2025-07-09 11:55:00
Dataflow Cost Calculator,"A Co-presentation with Google and Exabeam, demonstrating how Dataflow APIs and other GCP products were used in conjunction with Beam metrics to minimize infrastructure costs. ","Svetak Sundhar, Aditya Saraf",25 minutes,,Horizon Hall,2025-07-09 12:00:00,25,2025-07-09 12:25:00
"Become a Contributor: Making Changes, Running Patched Pipeline, and Contributing back to Beam","Apache Beam as an open-sourced product builts on community contributions. This talk shares how to make changes to Beam, then run your pipeline with patched Beam SDK, and finally, check in your changes to Beam. The talk also introduces recent efforts of making contribution and deploy of  custom Beam easier.

Reference: https://github.com/apache/beam/blob/master/contributor-docs/code-change-guide.md",Yi Hu,25 minutes,Ecosystem & Community,Palisades,2025-07-09 12:30:00,25,2025-07-09 12:55:00
Scalable Prompt Optimization in Apache Beam LLM Workflows,"As Large Language Models (LLMs) become integral to data pipelines, optimizing prompts at scale is critical for consistency, cost control, and performance. In this session, you’ll learn how to embed prompt-tuning and dynamic prompt-generation into an LLM workflow that is executed as Apache Beam pipeline.",Tomi Ajakaiye,25 minutes,"Scalability & Performance, Emerging trends",Horizon Hall,2025-07-09 12:30:00,25,2025-07-09 12:55:00
"Build Seamless Data Ecosystems: Real-World Integrations with Apache Beam, Kafka, and Iceberg","Modern data architectures are no longer built around a single tool — they thrive on interoperability and community-driven integration. This session explores how Apache Beam serves as the flexible processing engine that connects streaming platforms like Kafka with modern, ACID-compliant data lakehouse solutions like Apache Iceberg.

Through real-world architecture patterns and practical examples, we’ll dive into how organizations are using Beam to unify disparate data sources, enable real-time and batch analytics, and future-proof their data platforms. You'll also gain insights into how the open-source community continues to drive innovation across this ecosystem — from new connectors to performance optimizations and beyond.

Whether you're designing pipelines, modernizing ETL, or exploring community-powered tooling, this session gives you the blueprint to build scalable, production-ready data ecosystems with confidence.",Rajesh Vayyala,25 minutes,,Horizon Hall,2025-07-09 14:00:00,25,2025-07-09 14:25:00
See the Full Picture: Integrating Beam/Dataflow into Your Distributed Traces,"Achieving holistic observability often hits a wall at asynchronous batch or streaming systems. While OpenTelemetry provides standards for tracing, integrating systems like Apache Beam/Dataflow requires specific considerations. This presentation details the successful integration of Beam pipelines with OpenTelemetry's tracing APIs. We'll explore the mechanisms for context propagation across Beam's distributed workers and stages, enabling pipelines to join traces initiated by upstream services. Discover how spans generated within the Dataflow runner can be exported and visualized alongside the rest of your application traces in Google Cloud Trace, finally delivering the ""full picture"" of your system's behavior, including its data processing components.",Radek Stankiewicz,25 minutes,Connecting disparate systems with modern data architectures,Horizon Hall,2025-07-09 15:00:00,25,2025-07-09 15:25:00
Revisiting Splittable DoFn in KafkaIO,"SDF and IO are tricky subjects, doubly so for SDF IO. This session dives deeper into the SDF read transform in KafkaIO and will highlight a few crucial performance issues that have been addressed in recent releases of Apache Beam. The session is aimed at IO contributors with the goal to emphasize the subtle nuances in SDF and IO code that can make or break performance.",Steven van Rossum,50 minutes,"Real-time data applications, Scalability & Performance",Palisades,2025-07-09 15:00:00,50,2025-07-09 15:50:00
"Real-Time Predictive Modeling with MLServer, MLFlow, and Apache Beam","Oden Technologies delivers real-time machine learning to manufacturing environments with Apache Beam. In this session, we'll demonstrate how Oden aggregates hundreds of sensor streams into real-time tensors for predictive scoring against SKLearn pipelines hosted on MLServer. We'll also discuss how we use MLFlow for model management and monitoring, along with the infrastructure we've developed to coordinate these systems, enabling reliable model testing, deployment, and code updates.","Devon Peticolas, Jeswanth Yadagani",25 minutes,"Real-time data applications, Connecting disparate systems with modern data architectures, Emerging trends",Horizon Hall,2025-07-09 16:00:00,25,2025-07-09 16:25:00